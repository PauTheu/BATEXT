%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Appendix %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Appendix}
\label{ch:appendix}

\section{Code Datenbasis}
\label{appendix:daten}

01\_Datenbasis.ipynb

\section{Code deskriptive Statikstiken}
\label{appendix:stat}

02\_Statistiken.ipynb

\section{Code Kapitel \ref{ch:jaffe}}
\label{appendix:jaffe}

03\_TechnologieraumPatentklassen.ipynb

\section{Code Kapitel \ref{ch:zitat}}
\label{appendix:stuart}

04\_TechnologieraumPatentzitate.ipynb

\section{Code Kapitel \ref{ch:tm}}
\label{appendix:tm}

05\_TechnologieraumPatenttexte.ipynb

\section{Berechnung der Omegawerte}
\label{appendix:omega}


\vphantom{dasddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddd}

\begin{lstlisting}[style=python]

# Berechnet die Distanz zweier Firmen nach Kitahara und Oikawa (2017)
def omega(z0,z1,eta=0.5):
    
    '''
    z0, z1 - Zitate von Firma_0 und Firma_1 als dictionary,
    eta - technologischer Diskontierungsfaktor,
    zitate2 - Zitate der zweiten Ebene als dictionary - gibt an welches Patent zitiert,
    zitate2R - Rückrichtung der Zitate auf der zweiten Ebene als dictionary - gibt an von welchem Patente zitiert wird
    '''
    
    '''
    In diesem Teil des Codes wird nur der Omegatyp und die Gewichtung der Patente bestimmt, Mehrfachzitate werden bei der Berechnung der Omegawerte
    im zweiten Teil des Codes berücksichtigt
    '''
    
    # omega^1, Schnittmenge, erste Zitierebene
    ov = set(z0) & set(z1) 
    
    # Übrige Patente (~P_ij und ~P_ji)
    A0 = set(z0) - ov
    A1 = set(z1) - ov
    
    # Gewichtung der omega^1
    g = dict()
    for p in ov:
        g[p] = (1,0)
        
    # omega^21 und omega^22 in Richtung 01 (ij), Gewichtung    
    for p in A0:
        gewicht = 0
        typ = 0
        if p in zitate2:
            a = zitate2[p] - ov # Einschränkung der Menge auf noch nicht berechnete Zitate (~P'_ij,)
            schnitt = a & A1 
            if len(schnitt) > 0:    # omega^21-typ
                gewicht = len(schnitt)/len(a)
                typ = 1
            else:                   # evtl omega^22-typ
                for q in a:
                    if q in zitate2R: 
                        qv = zitate2R[q]
                        schnitt = qv & A1
                        gewicht += len(schnitt)/len(a)
                        typ = 2

            if gewicht > 0:
                g[p] = (gewicht,typ)
    
    # omega^21 und omega^22 in Richtung 10 (ji), Gewichtung 
    for p in A1:
        gewicht = 0
        typ = 0
        if p in zitate2:
            a = zitate2[p] - ov
            schnitt = a & A0
            if len(schnitt) > 0:    # omega^21-typ
                gewicht = len(schnitt)/len(a)
                typ = 3
            else:                   # evtl omega^22-typ
                for q in a:
                    if q in zitate2R: 
                        qv = zitate2R[q]
                        schnitt = qv & A0
                        gewicht += len(schnitt)/len(a)
                        typ = 4

            if gewicht > 0:
                g[p] = (gewicht,typ)
         
        
    '''
    In zweiten Teil des Codes berechnen wir die verschiedenen Omegawerte anhand ihrer Gewichtung
    '''
        
        
    om01_1 = sum([g[k][0]*v for (k,v) in z0.items() if k in g and g[k][1] == 0])
    om01_1 += sum([g[k][0]*v for (k,v) in z1.items() if k in g and g[k][1] == 0])    
    om01_21 = sum([g[k][0]*v for (k,v) in z0.items() if k in g and g[k][1] == 1])
    om01_22 = sum([g[k][0]*v for (k,v) in z0.items() if k in g and g[k][1] == 2])
    om10_21 = sum([g[k][0]*v for (k,v) in z1.items() if k in g and g[k][1] == 3])
    om10_22 = sum([g[k][0]*v for (k,v) in z1.items() if k in g and g[k][1] == 4])
    
    # Berechnung des finalen Omegawertes
    om = om01_1 + eta * (om01_21 + om10_21) + eta * eta * (om01_22 + om10_22)
    nenner = sum(x for x in z0.values()) + sum(x for x in z1.values())
  
    # Berechnung der Distanz d_ij
    return  -np.log(om/nenner)
    
    
\end{lstlisting}

\section{Stopwords}
\label{appendix:stop}
['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't", 'wherein', 'furthermore', 'solution', 'problem', 'solve', 'method']

\section{SpaCy}
\label{appendix:spaCy}
\begin{lstlisting}[style=python]
def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    """https://spacy.io/api/annotation"""
    texts_out = []
    for sent in texts:
        doc = nlp(" ".join(sent)) 
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    return texts_out

def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]
\end{lstlisting}


\vphantom{dasddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddd}